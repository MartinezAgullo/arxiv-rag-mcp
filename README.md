# ArXiv RAG MCP Agent

A containerized agentic pipeline using **Model Context Protocol (MCP)** servers for academic literature retrieval and question-answering.

## ğŸ¯ Project Overview

**Phase 1 - Ingestion**: Search ArXiv papers â†’ Scrape content â†’ Chunk text â†’ Embed & store in Pinecone  
**Phase 2 - Query**: Retrieve relevant context â†’ Generate answer with GPT-4 â†’ Log to Notion â†’ Save locally

---

## ğŸ”§ MCP Servers Used

### 1. **ArXiv MCP Server**
Search, download, and read academic papers from ArXiv.

```json
{
  "command": "uv",
  "args": ["tool", "run", "arxiv-mcp-server", "--storage-path", "/app/data/arxiv_papers"]
}
```

**Tools**: `search_papers`, `download_paper`, `read_paper`, `list_papers`

---

### 2. **Pinecone MCP**
Vector database for semantic search with integrated embeddings.

```json
{
  "command": "npx",
  "args": ["-y", "@pinecone-database/mcp"],
  "env": {
    "PINECONE_API_KEY": "${PINECONE_API_KEY}"
  }
}
```

**Index Configuration**:
- **Name**: `arxiv-papers`
- **Model**: `llama-text-embed-v2` (NVIDIA-hosted, free tier)
- **Dimension**: 1024
- **Metric**: cosine
- **Cloud**: AWS (us-east-1)

**Tools**: `create-index-for-model`, `upsert-records`, `query-index`, `describe-index-stats`

---

### 3. **Notion MCP**
Log query interactions to Notion database.

```json
{
  "command": "npx",
  "args": ["-y", "@notionhq/notion-mcp-server"],
  "env": {
    "NOTION_TOKEN": "${NOTION_TOKEN}"
  }
}
```

**Database Schema**:
- **Query** (Title) - The user's question
- **Timestamp** (Date) - When the query was made
- **Answer** (Rich Text) - GPT-4 generated answer
- **Sources** (Rich Text) - Top retrieved paper chunks

**Setup**: Create integration at https://www.notion.so/my-integrations and share database with it.

---

### 4. **Filesystem MCP**
Save final outputs locally.

```json
{
  "command": "npx",
  "args": ["-y", "@modelcontextprotocol/server-filesystem", "/app/outputs"]
}
```

**Tools**: `read_file`, `write_file`, `list_directory`, `move_file`

---

## ğŸš€ Quick Start

### 1. Prerequisites
- Docker & Docker Compose
- API Keys:
  - OpenAI (GPT-4)
  - Pinecone
  - Notion integration token

### 2. Setup

```bash
# Clone repository
git clone <your-repo-url>
cd arxiv-rag-mcp

# Create .env file
touch .env
# Edit .env with your API keys

# Create Pinecone index (via web UI):
# - Name: arxiv-papers
# - Model: llama-text-embed-v2
# - Dimension: 1024, Metric: cosine

# Create Notion database with 4 columns:
# Query (Title), Timestamp (Date), Answer (Rich Text), Sources (Rich Text)
# Share it with your integration
```

### 3. Run

```bash
# Build and run both phases
docker-compose up

# Or run phases separately:

# Phase 1 only (ingestion)
docker-compose run --rm -e PHASE=ingestion arxiv-rag-agent

# Phase 2 only (query)
docker-compose run --rm -e PHASE=query -e USER_QUERY="Your question here" arxiv-rag-agent
```

### 4. View Results

```bash
# Check generated answer
cat outputs/answer.md

# View logs
docker-compose logs -f
```

---

## âš™ï¸ Configuration

Edit `.env` file:

```bash
# Required
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX_NAME=arxiv-papers
NOTION_TOKEN=ntn_...
NOTION_DATABASE_ID=...

# Optional
SEARCH_TOPIC="Large Language Model Reasoning"
MAX_PAPERS=10
ARXIV_CATEGORIES=cs.AI,cs.CL,cs.LG  # Leave empty for all categories
PHASE=both  # ingestion, query, or both
```

**Category Examples**:
- AI/ML: `cs.AI,cs.CL,cs.LG`
- Physics: `hep-ph,hep-th,hep-ex`
- Biology: `q-bio.BM,q-bio.NC`
- Leave empty to search all ArXiv categories

---

## ğŸ“ Project Structure

```
arxiv-rag-mcp/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ mcp_manager.py
â”‚   â”œâ”€â”€ phase1_ingestion.py
â”‚   â””â”€â”€ phase2_query.py
â”œâ”€â”€ data/arxiv_papers/    # Downloaded papers
â”œâ”€â”€ outputs/              # Generated answers
â””â”€â”€ logs/                 # Application logs
```

---

## ğŸ“ License

This project is licensed under the **MIT License**.

See [LICENSE](LICENSE) file for details.
